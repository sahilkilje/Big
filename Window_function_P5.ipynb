{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kLawRhM8B8v",
        "outputId": "2d5e5401-5a67-4ff9-defc-9ea40ad5682d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 57 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 55.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845514 sha256=892d54c48396c58c9f8a41b857a421dc10f91fff44864d52d3f91ac4d7920c17\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "Yyw1ptSO8JDr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SparkWindowFunction').getOrCreate()"
      ],
      "metadata": {
        "id": "WG0j09IF8hBd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Sumit\", \"Sales\", 4600),  \\\n",
        "    (\"Rohit\", \"Sales\", 4100),   \\\n",
        "    (\"Mitesh\", \"Finance\", 3000),  \\\n",
        "    (\"Gursheen\", \"Sales\", 3000),    \\\n",
        "    (\"Yash\", \"Finance\", 3300),  \\\n",
        "    (\"Ben\", \"Finance\", 3900),    \\\n",
        "    (\"Pnadey\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Shah\", \"Sales\", 4100) \\\n",
        "  )"
      ],
      "metadata": {
        "id": "9bBKLiMr8rhT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns= [\"employee_name\", \"department\", \"salary\"]"
      ],
      "metadata": {
        "id": "-3eUYAE49HFf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data = simpleData, schema = columns)"
      ],
      "metadata": {
        "id": "NQhy1XYW-TQD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23KIf_7Q-V8a",
        "outputId": "9e569e73-4a0e-4f17-a10e-e28893f9a1d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Sumit        |Sales     |4600  |\n",
            "|Rohit        |Sales     |4100  |\n",
            "|Mitesh       |Finance   |3000  |\n",
            "|Gursheen     |Sales     |3000  |\n",
            "|Yash         |Finance   |3300  |\n",
            "|Ben          |Finance   |3900  |\n",
            "|Pnadey       |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Shah         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number"
      ],
      "metadata": {
        "id": "yjtTuC8n-X5q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")"
      ],
      "metadata": {
        "id": "hLcku4yj-fkq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6e089mC-iyL",
        "outputId": "fb997b38-6dc3-4912-ff91-75ddd42cceca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Mitesh       |Finance   |3000  |1         |\n",
            "|Yash         |Finance   |3300  |2         |\n",
            "|Ben          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Pnadey       |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|Gursheen     |Sales     |3000  |2         |\n",
            "|Rohit        |Sales     |4100  |3         |\n",
            "|Shah         |Sales     |4100  |4         |\n",
            "|Sumit        |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHE4B98P-nPi",
        "outputId": "c31261b0-fe51-40c3-eb63-08ec0ccc9a9a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|       Mitesh|   Finance|  3000|   1|\n",
            "|         Yash|   Finance|  3300|   2|\n",
            "|          Ben|   Finance|  3900|   3|\n",
            "|        Kumar| Marketing|  2000|   1|\n",
            "|       Pnadey| Marketing|  3000|   2|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|     Gursheen|     Sales|  3000|   1|\n",
            "|        Rohit|     Sales|  4100|   3|\n",
            "|         Shah|     Sales|  4100|   3|\n",
            "|        Sumit|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dense_rank\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGHuGN8C-1US",
        "outputId": "80fbeb1b-5b46-42ca-ee52-e3e2a5c3ddc6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|       Mitesh|   Finance|  3000|         1|\n",
            "|         Yash|   Finance|  3300|         2|\n",
            "|          Ben|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|       Pnadey| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|     Gursheen|     Sales|  3000|         1|\n",
            "|        Rohit|     Sales|  4100|         2|\n",
            "|         Shah|     Sales|  4100|         2|\n",
            "|        Sumit|     Sales|  4600|         3|\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import percent_rank\n",
        "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjHR-dXy-8os",
        "outputId": "f6af0bed-1c3b-4c7a-b59a-b757ae435f93"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------+\n",
            "|employee_name|department|salary|percent_rank|\n",
            "+-------------+----------+------+------------+\n",
            "|       Mitesh|   Finance|  3000|         0.0|\n",
            "|         Yash|   Finance|  3300|         0.5|\n",
            "|          Ben|   Finance|  3900|         1.0|\n",
            "|        Kumar| Marketing|  2000|         0.0|\n",
            "|       Pnadey| Marketing|  3000|         1.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|     Gursheen|     Sales|  3000|         0.0|\n",
            "|        Rohit|     Sales|  4100|         0.5|\n",
            "|         Shah|     Sales|  4100|         0.5|\n",
            "|        Sumit|     Sales|  4600|         1.0|\n",
            "+-------------+----------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import ntile\n",
        "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kch29uLk_AUW",
        "outputId": "fee859d9-bc95-488e-b275-2fcb82cc928b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+-----+\n",
            "|employee_name|department|salary|ntile|\n",
            "+-------------+----------+------+-----+\n",
            "|       Mitesh|   Finance|  3000|    1|\n",
            "|         Yash|   Finance|  3300|    1|\n",
            "|          Ben|   Finance|  3900|    2|\n",
            "|        Kumar| Marketing|  2000|    1|\n",
            "|       Pnadey| Marketing|  3000|    2|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|     Gursheen|     Sales|  3000|    1|\n",
            "|        Rohit|     Sales|  4100|    1|\n",
            "|         Shah|     Sales|  4100|    2|\n",
            "|        Sumit|     Sales|  4600|    2|\n",
            "+-------------+----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import cume_dist    \n",
        "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GWSqI0__EEe",
        "outputId": "673917cf-ac71-446b-cf98-c8aff601d8e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------------+\n",
            "|employee_name|department|salary|         cume_dist|\n",
            "+-------------+----------+------+------------------+\n",
            "|       Mitesh|   Finance|  3000|0.3333333333333333|\n",
            "|         Yash|   Finance|  3300|0.6666666666666666|\n",
            "|          Ben|   Finance|  3900|               1.0|\n",
            "|        Kumar| Marketing|  2000|               0.5|\n",
            "|       Pnadey| Marketing|  3000|               1.0|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|     Gursheen|     Sales|  3000|               0.4|\n",
            "|        Rohit|     Sales|  4100|               0.8|\n",
            "|         Shah|     Sales|  4100|               0.8|\n",
            "|        Sumit|     Sales|  4600|               1.0|\n",
            "+-------------+----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
        "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul8nqfVx_IMe",
        "outputId": "ec953d46-a742-4891-b276-cba1fd29925c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}